# core/host_agent.py
import yaml
import json
import os
import time
import threading
import traceback
import re
from openai import OpenAI
from core.memory import ConversationMemory
# [ä¿®æ”¹ç‚¹] å¯¼å…¥è§£è€¦åçš„å·¥å…·ç®±
from core.functools.tools import Toolbox
from core.rag_store import RAGStore
# [ä¿®æ”¹ç‚¹] å¯¼å…¥ SentinelEngine
from core.sentinel_engine import SentinelEngine
from core.skill_manager import SkillManager  # [æ–°å¢]

DEBUG = True

class HostAgent:
    def __init__(self, default_session="resonance_main", config_path="config/config.yaml"):
        # [ä¿®æ”¹ç‚¹] é»˜è®¤ä¼šè¯ID
        self.default_session_id = default_session
        self.active_session_id = default_session # ä»…ç”¨äº backward compatibility
        
        # [æ–°å¢] ä¼šè¯çº§ä¸­æ–­äº‹ä»¶å­—å…¸ {session_id: threading.Event}
        self.interrupt_events = {}
        
        # --- [å…³é”®ä¿®æ”¹å¼€å§‹] è·¯å¾„é”šå®šä¿®å¤ ---
        # è·å–å½“å‰ host_agent.py çš„ç»å¯¹è·¯å¾„: .../backend/core/host_agent.py
        current_file_path = os.path.abspath(__file__)
        # è·å– backend ç›®å½•: .../backend
        backend_root = os.path.dirname(os.path.dirname(current_file_path))
        
        # å¼ºåˆ¶å°† config è·¯å¾„é”šå®šåˆ° backend ç›®å½•
        self.config_path = os.path.join(backend_root, config_path)
        self.profiles_path = os.path.join(backend_root, "config/profiles.yaml")
        self.user_profile_path = os.path.join(backend_root, "config/user_profile.yaml")
        
        # åŠ è½½æ‰€æœ‰é…ç½®
        self.load_all_configs()

        # [å…³é”®ä¿®æ”¹] å¼ºåˆ¶è®¡ç®— Vector Store çš„ç»å¯¹è·¯å¾„
        # æ— è®º config å†™çš„æ˜¯ä»€ä¹ˆç›¸å¯¹è·¯å¾„ï¼Œæˆ‘ä»¬éƒ½å°†å…¶è§£æä¸ºåŸºäº backend çš„ç»å¯¹è·¯å¾„
        raw_vec_path = self.config.get('system', {}).get('memory', {}).get('vector_store_path', './logs/vector_store')
        if not os.path.isabs(raw_vec_path):
            # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œæ‹¼æ¥åˆ° backend_root ä¸‹
            vec_path = os.path.normpath(os.path.join(backend_root, raw_vec_path))
        else:
            vec_path = raw_vec_path

        print(f"[System]: Memory Database Path Anchored to: {vec_path}") # æ‰“å°å‡ºæ¥ç¡®è®¤

        self.stop_flag = False
        self.memory_cache = {}
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“ (RAG)
        self.rag_store = RAGStore(persistence_path=vec_path)

        # [ä¿®å¤ Bug â‘ ] åˆå§‹åŒ– SentinelEngineï¼Œä½¿å…¶æˆä¸º HostAgent çš„å±æ€§
        # è¿™é‡Œçš„è·¯å¾„ä¹Ÿåº”è¯¥é”šå®šåˆ° backend
        sentinel_config_path = os.path.join(backend_root, "config/sentinels.json")
        self.sentinel_engine = SentinelEngine(config_path=sentinel_config_path)

        # [æ–°å¢] åˆå§‹åŒ– SkillManager
        self.skill_manager = SkillManager(self)

        # [ä¿®å¤ Bug â‘¡] åˆå§‹åŒ– active_skill çŠ¶æ€ï¼Œé˜²æ­¢ AttributeError
        # è¿™æ˜¯"è®¤çŸ¥è´Ÿè·ç®¡ç†"çš„æ ¸å¿ƒçŠ¶æ€ï¼šå½“å‰èšç„¦çš„æŠ€èƒ½
        self.active_skill = None 

        # å·¥å…·ç®±
        self.toolbox = Toolbox(self)
        
        # åˆå§‹åŒ– LLM Client
        self.client = None
        self._init_client()
        self.interrupt_events = {}
        self.active_session_id = default_session
        self.memory_cache = {}


    def get_memory(self, session_id=None) -> ConversationMemory:
        """[æ–°å¢] è·å–æŒ‡å®šä¼šè¯çš„å†…å­˜å¯¹è±¡ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºå¹¶ç¼“å­˜"""
        sid = session_id or self.active_session_id
        if sid not in self.memory_cache:
            win_size = self.config.get('system', {}).get('memory', {}).get('window_size', 10)
            self.memory_cache[sid] = ConversationMemory(session_id=sid, window_size=win_size)
        return self.memory_cache[sid]

    @property
    def memory(self):
        return self.get_memory(self.active_session_id)

    def load_all_configs(self):
        """åŠ è½½ç³»ç»Ÿé…ç½®ã€æ¨¡å‹é…ç½®å’Œç”¨æˆ·ç”»åƒ"""
        # 1. åŠ è½½ä¸»é…ç½®
        if os.path.exists(self.config_path):
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
        else:
            print(f"[Critical Warning] Config not found at {self.config_path}")
            self.config = {}
            
        # 2. åŠ è½½æ¨¡å‹ Profiles
        if os.path.exists(self.profiles_path):
            with open(self.profiles_path, 'r', encoding='utf-8') as f:
                self.profiles = yaml.safe_load(f).get('profiles', {})
        else:
            self.profiles = {}
            
        # 3. åŠ è½½ç”¨æˆ·ç”»åƒ
        if os.path.exists(self.user_profile_path):
            with open(self.user_profile_path, 'r', encoding='utf-8') as f:
                self.user_data = yaml.safe_load(f)
        else:
            self.user_data = {}

    def _init_client(self):
        """æ ¹æ® active_profile åˆå§‹åŒ– LLM å®¢æˆ·ç«¯"""
        active_id = self.config.get('active_profile')
        
        # å®¹é”™å¤„ç†ï¼šå¦‚æœæ‰¾ä¸åˆ° profileï¼Œä½¿ç”¨é»˜è®¤æˆ–ç©ºé…ç½®
        if active_id not in self.profiles:
            print(f"[Warning] Profile '{active_id}' not found in profiles.yaml. Using safe defaults.")
            self.current_model_config = {
                'api_key': 'EMPTY',
                'base_url': None,
                'model': 'gpt-3.5-turbo',
                'temperature': 0.7
            }
        else:
            self.current_model_config = self.profiles[active_id]
        
        self.base_url = self.current_model_config.get('base_url')
        self.api_key = self.current_model_config.get('api_key')

        # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        try:
            self.client = OpenAI(
                base_url=self.base_url,
                api_key=self.api_key,
                timeout=60.0,  # è®¾ç½®è¶…æ—¶é˜²æ­¢æ— é™ç­‰å¾…
                max_retries=2
            )
            print(f"[LLM Init]: Client configured. URL: {self.base_url}, Model: {self.current_model_config.get('model')}")
        except Exception as e:
            print(f"[LLM Error]: Failed to initialize OpenAI client: {e}")
            # å³ä½¿å¤±è´¥ï¼Œä¹Ÿå®šä¹‰ä¸º Noneï¼Œé˜²æ­¢ AttributeErrorï¼Œå¹¶åœ¨ chat ä¸­å¤„ç†
            self.client = None
    def activate_skill(self, skill_name):
        """
        [State Change] æ¿€æ´»ä¸€ä¸ªæŠ€èƒ½ (Activation Phase)ã€‚
        1. æ£€æŸ¥æ˜¯å¦å­˜åœ¨ã€‚
        2. è®¾ç½® self.active_skillã€‚
        3. ä¸‹ä¸€æ¬¡ _build_dynamic_system_prompt æ—¶ä¼šæ³¨å…¥ SOPã€‚
        """
        # å°è¯•åŠ è½½ SOP ä»¥éªŒè¯æŠ€èƒ½æ˜¯å¦å­˜åœ¨
        sop_text, _ = self.skill_manager.load_skill_context(skill_name)
        if not sop_text:
            return f"Error: Skill '{skill_name}' not found or failed to load."
        
        self.active_skill = skill_name
        return f"SUCCESS: Skill '{skill_name}' activated. SOP instructions loaded. Exclusive tools are now visible."

    def deactivate_skill(self):
        """
        [State Change] é€€å‡ºæŠ€èƒ½æ¨¡å¼ï¼Œå›åˆ°é€šç”¨æ¨¡å¼ã€‚
        """
        prev_skill = self.active_skill
        self.active_skill = None
        return f"Skill '{prev_skill}' deactivated. Returned to General Mode."

    def _build_dynamic_system_prompt(self, relevant_memories: list, memory_instance: ConversationMemory, original_query: str = None):
        """
        æ„å»ºé«˜çº§ç»“æ„åŒ– Prompt
        åŒ…å«ï¼šèº«ä»½ã€å·¥å…·èƒ½åŠ›ã€ç”¨æˆ·ç”»åƒã€é•¿æœŸè®°å¿†(RAG)ã€å½“å‰å¯¹è¯æ‘˜è¦
        """
        # 1. åŸºç¡€èº«ä»½è®¾å®š
        base_identity = """
You are Resonance, an advanced Windows AI Host.

### CORE OPERATING PROTOCOLS (MUST FOLLOW):

1.  **PLAN FIRST (MANDATORY)**: 
    For ANY task that is not a simple greeting, you MUST start your response with a structured plan block using the `<plan>` XML tag.
    
    Format:
    <plan>
    - [ ] Step 1: Description
    - [ ] Step 2: Description (Deliverable: filename.ext)
    </plan>

    *Update this plan in subsequent turns by marking items as [x].*

2.  **DELIVERABLE AWARENESS**: 
    Know exactly what files or results you need to produce. Do not stop until the final deliverable is created and verified.

3.  **TOOL USAGE**:
    - Use `list_directory_files` before reading/writing to understand the path.
    - Use `read_file_content` to check content before editing.
    - If a tool fails, analyze the error and try a different approach.

5. **Active Memory.** You have access to a long-term Vector Memory. 
   - You can query it using `search_long_term_memory` if the context is missing.
   - You can SAVE important findings using `add_long_term_memory`.
   - You can DELETE obsolete facts using `delete_long_term_memory`.

6. **Anthropics Skills.**    
    - To use a specialized capability, use 'manage_skills' to ACTIVATE it first.
    - Once active, follow the SOP RIGIDLY.

Tool Use Reminders:
You have a limit on how many tools you can use in one session. Use them wisely.
If you hit the limit, you will be given a chance to reflect and continue if necessary.
Or, if user continue to chat with you, the limit will be reset too.
"""
        # 2. é”šå®šåŸå§‹è¯·æ±‚
        anchor_section = ""
        if original_query:
            anchor_section = f"\n### CURRENT MISSION ANCHOR\nUser's Original Request: \"{original_query}\"\n(Align all actions to complete this specific request. Do not get distracted.)\n"

        # 3. æ³¨å…¥ç”¨æˆ·ç”»åƒ
        user_section = "\n### USER PROFILE\n"
        user_info = self.user_data.get('user_info', {})
        known_projects = self.user_data.get('known_projects', {})
        
        for k, v in user_info.items():
            user_section += f"- {k}: {v}\n"
        if known_projects:
            user_section += "- Known Projects:\n"
        for proj, path in known_projects.items():
            user_section += f"  * {proj}: {path}\n"

        # --- [å…³é”®ä¿®æ”¹] JIT SOP æ³¨å…¥ ---
        skill_section = ""
        if self.active_skill:
            # åªæœ‰å½“ Skill æ¿€æ´»æ—¶ï¼Œæ‰åŠ è½½ SOP (å‡å°‘ Tokenï¼Œé˜²æ­¢æ±¡æŸ“)
            sop_text, _ = self.skill_manager.load_skill_context(self.active_skill)
            if sop_text:
                skill_section = f"\n\n### ğŸ”¥ ACTIVE SKILL: {self.active_skill}\n{sop_text}\nFOLLOW THIS SOP RIGIDLY.\n"
        else:
            # æœªæ¿€æ´»æ—¶ï¼Œæç¤ºå¯ç”¨æŠ€èƒ½ç´¢å¼• (Discovery Phase)
            skill_index = self.skill_manager.get_skill_index()
            skill_section = f"\n### AVAILABLE SKILLS\n{skill_index}\n(Use 'manage_skills' to activate one if needed)\n"

        # 4. é•¿æœŸè®°å¿†æ³¨å…¥ (RAG Results)
        rag_section = ""
        if relevant_memories:
            rag_section = "\n### Long-term Memories (Reference Only)\n"
            for mem in relevant_memories:
                rag_section += f"- {mem}\n"
            rag_section += "(Use these ONLY if they help the *Current* Original Intent.)\n"

        # [ä¿®æ”¹ç‚¹] ä½¿ç”¨ä¼ å…¥çš„ memory_instance
        summary_text = memory_instance.load_summary()
        summary_section = ""
        if summary_text:
            summary_section = f"\n### PREVIOUS CONVERSATION SUMMARY\n{summary_text}\n"

        # ç»„åˆ Prompt
        full_prompt = base_identity + anchor_section + user_section + skill_section + rag_section + summary_section

        if DEBUG:
            print(f"[DEBUG] Full Prompt:{full_prompt}")
        return full_prompt

    def _update_summary_if_needed(self, memory_instance: ConversationMemory):
        """[æ‘˜è¦æœºåˆ¶] æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©å†å²è®°å½•"""
        if not self.config['system'].get('memory', {}).get('enable_summary', True):
            return

        full_log = memory_instance.get_full_log()
        if len(full_log) > 0 and len(full_log) % 10 == 0:
            text_to_summarize = memory_instance.get_messages_for_summarization()
            if not text_to_summarize:
                return

            current_summary = memory_instance.load_summary()
            
            # ä½¿ç”¨ LLM ç”Ÿæˆæ‘˜è¦
            try:
                # [BUG FIX Check] Ensure client exists before summarizing
                if not self.client:
                    return

                prompt = f"""
                You are a memory compressor.
                
                Current Summary:
                {current_summary}
                
                New Conversation Log to Append:
                {text_to_summarize}
                
                Task: Update the summary to include the key information from the new log. Keep it concise.
                Return ONLY the updated summary text.
                """
                
                response = self.client.chat.completions.create(
                    model=self.current_model_config['model'],
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.3
                )
                
                new_summary = response.choices[0].message.content
                self.memory.save_summary(new_summary)
                print(f"[System]: Memory summarized. Length: {len(new_summary)}")
                print(f"[System]: Memory preview: {new_summary}")
            except Exception as e:
                print(f"[Warning] Failed to generate summary: {e}")

    # =========================================================================
    # [æ–°å¢æ ¸å¿ƒé€»è¾‘] å¼‚æ­¥è®°å¿†èƒå–ä¸å­˜å‚¨
    # =========================================================================
    def _extract_and_save_memory_async(self, turn_events_log, session_id):
        """
        åå°çº¿ç¨‹ä»»åŠ¡ï¼šåˆ†æå¯¹è¯ï¼Œèƒå–æœ‰ä»·å€¼çš„ä¿¡æ¯å­˜å…¥å‘é‡åº“ã€‚
        é¿å…å°†åƒåœ¾å¯¹è¯ï¼ˆ"ä½ å¥½", "å—¯"ï¼‰å­˜å…¥ã€‚
        æ³¨æ„ï¼šç°åœ¨æˆ‘ä»¬ä¹Ÿæœ‰ Active Memory å·¥å…·ï¼Œä¸¤è€…å¹¶è¡Œã€‚
        """
        try:
            # [BUG FIX Check]
            if not self.client:
                return

            # è°ƒç”¨ LLM è¿›è¡Œä¿¡æ¯èƒå– (Extraction)
            # ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹æˆ–ç›¸åŒçš„æ¨¡å‹ï¼ŒPrompt ä¾§é‡äº"äº‹å®æå–"
            extraction_prompt = f"""
You are a Memory Extractor. Analyze the following interaction Turn (User input, AI thoughts, and Tool outputs).
Your goal is to extract NEW, PERMANENT facts about the user, their projects, or technical solutions found.

[Interaction Turn Log]:
{turn_events_log}

[Instructions]:
1. Focus on: Project paths, User preferences, recurring technical issues/solutions, specific facts.
2. Ignore: Transient states (e.g., current CPU usage), casual greetings, or "OK" messages.
3. If no permanent fact is found, output "NO_INFO".
4. If facts are found, output them as concise, independent statements.
5. Example Output: "The user's project 'Resonance' is located at D:\\Develop\\Resonance."

[Output]:
"""
            response = self.client.chat.completions.create(
                model=self.current_model_config['model'],
                messages=[{"role": "user", "content": extraction_prompt}],
                temperature=0.1,
                max_tokens=256
            )
            
            extracted_info = response.choices[0].message.content.strip()
            
            # 3. å­˜å‚¨é€»è¾‘
            if extracted_info and "NO_INFO" not in extracted_info:
                # å­˜å…¥å‘é‡åº“
                success = self.rag_store.add_memory(
                    text=extracted_info,
                    metadata={
                        "type": "conversation_insight",
                        "session": session_id,
                        "original_user_input": turn_events_log[:50]
                    }
                )
                if success:
                    # åœ¨æ—¥å¿—ä¸­é™é»˜è®°å½•ï¼Œç”¨äºè°ƒè¯•ï¼Œä¸å¹²æ‰°ä¸»çº¿ç¨‹è¾“å‡º
                    print(f"[Memory System]: Auto Memory Extracted. Archived -> {extracted_info}")
                    pass

        except Exception as e:
            # è¿™é‡Œçš„å¼‚å¸¸ç»å¯¹ä¸èƒ½å½±å“ä¸»çº¿ç¨‹
            print(f"[Memory System Error]: {e}")

    # [æ–°å¢] å¤–éƒ¨è°ƒç”¨ä¸­æ–­æ–¹æ³• (æ”¯æŒä¼šè¯çº§ä¸­æ–­)
    def interrupt(self, session_id=None):
        """è§¦å‘ä¸­æ–­ä¿¡å·"""
        if session_id:
            # ä¸­æ–­ç‰¹å®šä¼šè¯
            if session_id in self.interrupt_events:
                print(f"[System]: Interrupting session '{session_id}'")
                self.interrupt_events[session_id].set()
        else:
            # ä¸­æ–­æ‰€æœ‰ (ä¿ç•™æ—§è¡Œä¸º)
            print("[System]: Interrupting ALL sessions.")
            for evt in self.interrupt_events.values():
                evt.set()

    def handle_sentinel_trigger(self, message):
        """
        å½“å“¨å…µè§¦å‘æ—¶è°ƒç”¨ã€‚
        å°†æ¶ˆæ¯ä½œä¸º 'system' æˆ– 'tool' ç»“æœå†™å…¥ 'resonance_main' ä¼šè¯ã€‚
        """
        try:
            main_mem = self.get_memory("resonance_main")
            # åŠ ä¸Šæ—¶é—´æˆ³
            timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
            formatted_msg = f"[Sentinel Alert {timestamp}]: {message}"
            main_mem.add_system_message(formatted_msg)
            print(f"[Core] Injected sentinel alert into 'resonance_main' session.")
        except Exception as e:
            print(f"[Core Error] Failed to inject sentinel memory: {e}")

    # =========================================================================
    # [æ ¸å¿ƒæ–°å¢] Supervisor (ç£æˆ˜) æœºåˆ¶
    # =========================================================================
    def _supervisor_check(self, session_memory: ConversationMemory, user_input: str) -> dict:
        """
        ç£æˆ˜å‘˜æ£€æŸ¥ï¼šåˆ†æå½“å‰ä¸Šä¸‹æ–‡ï¼Œåˆ¤æ–­ä»»åŠ¡æ˜¯å¦çœŸæ­£å®Œæˆã€‚
        """
        print("[Supervisor]: Checking mission status...")
        
        # è·å–æœ€è¿‘çš„ä¸Šä¸‹æ–‡ï¼ˆåŒ…å« Plan å’Œ Executionï¼‰
        context = session_memory.get_active_context()[-5:] 
        context_str = json.dumps(context, ensure_ascii=False)
        
        supervisor_prompt = f"""
[SUPERVISOR PROTOCOL]
You are the Overwatch System. Your job is to verify if the AI has completed the user's request based on the plan.

Original Request: "{user_input}"
Recent History: {context_str}

Checklist:
1. Did the AI output a `<plan>`?
2. Are all items in the plan marked as completed (e.g., [x])?
3. Were the deliverables actually generated/modified?

If the task is incomplete or the AI is stopping prematurely, output:
{{"status": "INCOMPLETE", "instruction": "Briefly state what must be done next."}}

If the task is truly done or waiting for user input, output:
{{"status": "COMPLETE", "instruction": "None"}}

Response (JSON Only):
"""
        try:
            response = self.client.chat.completions.create(
                model=self.current_model_config['model'],
                messages=[{"role": "user", "content": supervisor_prompt}],
                temperature=0.1,
                response_format={"type": "json_object"}
            )
            decision = json.loads(response.choices[0].message.content)
            print(f"[Supervisor]: Decision -> {decision}")
            return decision
        except Exception as e:
            print(f"[Supervisor Error]: {e}")
            return {"status": "COMPLETE"} # å‡ºé”™æ—¶ä¿å®ˆå¤„ç†ï¼Œä¸é™·å…¥æ­»å¾ªç¯

    def chat(self, user_input, session_id="default"):
        """
        ä¸»äº¤äº’é€»è¾‘ (æ”¯æŒç£æˆ˜å¾ªç¯)
        """
        # [å¹¶å‘å®‰å…¨] åˆå§‹åŒ–è¯¥ä¼šè¯çš„ä¸­æ–­äº‹ä»¶
        if session_id not in self.interrupt_events:
            self.interrupt_events[session_id] = threading.Event()
        stop_event = self.interrupt_events[session_id]
        stop_event.clear() # é‡ç½®çŠ¶æ€

        # [å¹¶å‘å®‰å…¨] è·å–ä¼šè¯ä¸“å±å†…å­˜
        session_memory = self.get_memory(session_id)
        
        if self.client is None:
            yield {"type": "error", "content": "LLM Client is not initialized. Check profiles.yaml."}
            return

        try:
            # è®°å½•åˆå§‹ç”¨æˆ·æ¶ˆæ¯
            session_memory.add_user_message(user_input)
            
            # 2. æ£€ç´¢é•¿æœŸè®°å¿†
            top_k = self.config.get('system', {}).get('memory', {}).get('retrieve_top_k', 3)
            # [ä¿®æ”¹] è¯»å–é…ç½®çš„ç­–ç•¥ï¼Œé»˜è®¤ä¸º semantic
            rag_strategy = self.config.get('system', {}).get('memory', {}).get('rag_strategy', 'semantic')
            relevant_docs = self.rag_store.search_memory(user_input, n_results=top_k, strategy=rag_strategy)
            
            # ç£æˆ˜å¾ªç¯é™åˆ¶
            MAX_SUPERVISOR_LOOPS = 3
            supervisor_loops = 0
            
            while supervisor_loops <= MAX_SUPERVISOR_LOOPS:
                # 1. æ„å»º Prompt (Pinned Context é€»è¾‘åœ¨ memory.get_active_context ä¸­å¤„ç†)
                dynamic_sys_prompt = self._build_dynamic_system_prompt(relevant_docs, session_memory, original_query=user_input)
                messages = [{"role": "system", "content": dynamic_sys_prompt}] + session_memory.get_active_context()
                
                # 2. ReAct å¾ªç¯ (Action Loop)
                MAX_TOOL_ITERATIONS = 15
                current_iteration = 0
                turn_log_for_extraction = f"User Input: {user_input}\n"
                
                # æ ‡è®°æœ¬æ¬¡ç”Ÿæˆæ˜¯å¦çœŸæ­£ç»“æŸï¼ˆæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼‰
                is_generation_finished = False

                while current_iteration < MAX_TOOL_ITERATIONS:
                    # å¾ªç¯å¼€å§‹å‰æ£€æŸ¥æ‰“æ–­
                    if stop_event.is_set():
                        yield {"type": "status", "content": "â›” Task Interrupted."}
                        return

                    current_iteration += 1
                    
                    # åŠ¨æ€æ›´æ–°å·¥å…·
                    current_tools = self.toolbox.get_tool_definitions()
                    
                    # Stream Call
                    try:
                        response = self.client.chat.completions.create(
                            model=self.current_model_config['model'],
                            messages=messages,
                            tools=current_tools,
                            tool_choice="auto",
                            temperature=self.current_model_config['temperature'],
                            stream=True
                        )
                    except Exception as e:
                        yield {"type": "error", "content": f"LLM API Error: {str(e)}"}
                        return # å‘ç”ŸAPIé”™è¯¯ï¼Œåœæ­¢

                    full_response_content = ""
                    tool_calls_buffer = {} # ç”¨äºæ”¶é›†æµå¼çš„ tool_calls

                    try:
                        for chunk in response:
                            if stop_event.is_set():
                                response.close()
                                yield {"type": "status", "content": "\n[Stopped]"}
                                return 

                            delta = chunk.choices[0].delta
                            
                            if hasattr(delta, 'content') and delta.content is not None:
                                content_chunk = delta.content
                                full_response_content += content_chunk
                                yield {"type": "delta", "content": content_chunk}
                            
                            if delta.tool_calls:
                                for tc_chunk in delta.tool_calls:
                                    idx = tc_chunk.index
                                    if idx not in tool_calls_buffer:
                                        tool_calls_buffer[idx] = {
                                            "id": tc_chunk.id,
                                            "name": tc_chunk.function.name,
                                            "arguments": ""
                                        }
                                    if tc_chunk.function.arguments:
                                        tool_calls_buffer[idx]["arguments"] += tc_chunk.function.arguments
                    except Exception as e:
                        yield {"type": "error", "content": f"Stream context error: {str(e)}"}
                        return
                    
                    if full_response_content:
                        turn_log_for_extraction += f"AI Thought: {full_response_content}\n"

                    # æ£€æŸ¥æ‰“æ–­
                    if stop_event.is_set():
                        return

                    # å¤„ç† Tool Calls
                    active_tool_calls = []
                    for _, tc_data in tool_calls_buffer.items():
                        class Func:
                            def __init__(self, n, a): self.name, self.arguments = n, a
                        class TC:
                            def __init__(self, i, f): self.id, self.function = i, f
                        active_tool_calls.append(TC(tc_data["id"], Func(tc_data["name"], tc_data["arguments"])))

                    # è®°å½• AI æ¶ˆæ¯åˆ°å†…å­˜
                    if active_tool_calls:
                        session_memory.add_ai_tool_call(full_response_content, active_tool_calls)
                        messages.append({"role": "assistant", "content": full_response_content, "tool_calls": [
                            {"id": tc.id, "type": "function", "function": {"name": tc.function.name, "arguments": tc.function.arguments}}
                            for tc in active_tool_calls
                        ]})
                        
                        for tc in active_tool_calls:
                            if stop_event.is_set(): return
                            func_name = tc.function.name
                            try:
                                args = json.loads(tc.function.arguments)
                            except:
                                args = {}
                            
                            yield {"type": "status", "content": f"Executing: {func_name}..."}
                            tool_result_raw = self._route_tool_execution(func_name, args, stop_event)
                            
                            # å¢åŠ  Prompt æŒ‡å¼•
                            tool_result = f"{tool_result_raw}\n\n[System: Check your plan. Update <plan> status in next response.]"
                            
                            yield {"type": "tool", "name": func_name, "content": tool_result_raw}
                            session_memory.add_tool_message(tool_result, tc.id)
                            
                            messages.append({"role": "tool", "tool_call_id": tc.id, "content": tool_result})
                    else:
                        # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¯´æ˜ AI è®¤ä¸ºè¯¥å›åˆç»“æŸ
                        session_memory.add_ai_message(full_response_content)
                        is_generation_finished = True
                        break # è·³å‡º Action Loop

                # 3. ç£æˆ˜æ£€æŸ¥ (Supervisor Check)
                # åªæœ‰å½“ AI è®¤ä¸ºè‡ªå·±å®Œæˆäº†ï¼ˆè·³å‡ºäº† Action Loopï¼‰ï¼Œä¸”è¿˜æ²¡è¾¾åˆ° Supervisor é™åˆ¶æ—¶æ£€æŸ¥
                if is_generation_finished and supervisor_loops < MAX_SUPERVISOR_LOOPS:
                    decision = self._supervisor_check(session_memory, user_input)
                    
                    if decision.get("status") == "INCOMPLETE":
                        supervisor_loops += 1
                        instruction = decision.get("instruction", "Task incomplete.")
                        msg = f"[ğŸ‘® SUPERVISOR INTERVENTION]: Task not finished. {instruction} Continue executing the plan immediately."
                        
                        yield {"type": "status", "content": f"ğŸ‘® Supervisor: {instruction} (Auto-Continuing)"}
                        
                        # æ³¨å…¥ç³»ç»ŸæŒ‡ä»¤ï¼Œå¼ºåˆ¶ç»§ç»­
                        session_memory.add_system_message(msg)
                        # è¿™é‡Œä¸éœ€è¦æ›´æ–° messagesï¼Œå› ä¸ºå¤–å±‚ while ä¼šé‡æ–°æ„å»º Prompt (åŒ…å«æ–°æ³¨å…¥çš„ system msg)
                        continue # é‡æ–°è¿›å…¥ ReAct å¾ªç¯
                    else:
                        # ä»»åŠ¡å®Œæˆ
                        break 
                else:
                    # æ¨¡å‹è®¤ä¸ºä»»åŠ¡ç»“æŸæˆ–ä¸éœ€è¦ç»§ç»­
                    yield {"type": "status", "content": "âœ… Task reflection complete. Finishing."}
                    break

            # æœ€ç»ˆæ”¶å°¾
            self._update_summary_if_needed(session_memory)
            
            # 8. [ä¿®æ”¹ç‚¹] å¯åŠ¨å¼‚æ­¥çº¿ç¨‹è¿›è¡Œè®°å¿†èƒå–ä¸å‘é‡å­˜å‚¨
            # ä½¿ç”¨å®ˆæŠ¤çº¿ç¨‹ (daemon=True)ï¼Œä¸»ç¨‹åºé€€å‡ºæ—¶å®ƒè‡ªåŠ¨ç»“æŸï¼Œä¸ä¼šå¡æ­»è¿›ç¨‹
            memory_thread = threading.Thread(
                target=self._extract_and_save_memory_async,
                args=(turn_log_for_extraction, session_id), # ä¼ é€’ session_id
                daemon=True
            )
            memory_thread.start()


        except Exception as e:
            error_details = traceback.format_exc()
            print(error_details) # åœ¨æ§åˆ¶å°æ‰“å°è¯¦ç»†å †æ ˆ
            yield {"type": "error", "content": str(error_details)}
        finally:
            # æ¸…ç†äº‹ä»¶å¼•ç”¨ï¼ˆå¯é€‰ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼ï¼‰
            if session_id in self.interrupt_events:
                # ä»»åŠ¡ç»“æŸåå¹¶ä¸ä¸€å®šè¦åˆ é™¤ Eventï¼Œå¯ä»¥ç•™ç€å¤ç”¨ï¼Œåªè¦æ¯æ¬¡ chat start æ—¶ clear å³å¯
                pass

    def _route_tool_execution(self, function_name, args, stop_event=None):
        """
        è·¯ç”±å·¥å…·è°ƒç”¨åˆ° Toolbox
        """
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰é«˜ä¼˜å…ˆçº§çš„æ‰“æ–­
            if stop_event and stop_event.is_set():
                return "[System]: Tool execution cancelled."

            # [æ–°å¢] è·¯ç”± Active Memory Tools
            if function_name == "search_long_term_memory":
                return self.toolbox.search_long_term_memory(args.get("query"))
            elif function_name == "add_long_term_memory":
                return self.toolbox.add_long_term_memory(args.get("text"), args.get("tag"))
            elif function_name == "delete_long_term_memory":
                return self.toolbox.delete_long_term_memory(args.get("memory_id"))

            # ... Existing Routes ...
            if function_name == "manage_skills":
                return self.toolbox.manage_skills(args.get("action"), args.get("skill_name"))
            
            # 2. Learn Skills
            if function_name == "learn_new_skill":
                return self.toolbox.learn_new_skill(args.get("url_or_path"))

            # 2. åŠ¨æ€å¯¼å…¥çš„æŠ€èƒ½ (skill_*)
            if function_name.startswith("skill_"):
                # æ³¨æ„ï¼šskill_manager å¯èƒ½åœ¨åˆå§‹åŒ–æ—¶æ²¡å‡†å¤‡å¥½ï¼Œå¢åŠ é˜²æŠ¤
                if self.skill_manager:
                    return self.skill_manager.execute_skill(function_name, args)
                else:
                    return "Error: Skill Manager not initialized."

            # 3. é—ç•™è„šæœ¬
            if function_name == "invoke_legacy_script":
                return self.toolbox.invoke_registered_skill(args.get("alias"), args.get("args", ""), stop_event)


            elif function_name == "execute_shell_command":
                return self.toolbox.execute_shell(args.get("command"), stop_event=stop_event)
                

            elif function_name == "scan_directory_projects":
                # è¿™é‡Œåªè´Ÿè´£è¿”å›æ‰«æç»“æœå­—ç¬¦ä¸²
                return self.toolbox.scan_and_remember(args.get("path"))
                
            elif function_name == "read_file_content":
                return self.toolbox.read_file_content(args.get("file_path"))
                
            elif function_name == "remember_user_fact":
                # åªè´Ÿè´£æ›´æ–° UserProfile æ–‡ä»¶
                return self.toolbox.remember_user_fact(args.get("key"), args.get("value"))
                
            elif function_name == "list_directory_files":
                return self.toolbox.list_directory_files(
                    directory_path=args.get("directory_path"), 
                    recursive=args.get("recursive", True),
                    depth=args.get("depth", 2)
                )
                
            elif function_name == "search_files_by_keyword":
                # æœç´¢ä¹Ÿå¯èƒ½è€—æ—¶ï¼Œä¼ é€’ stop_event
                return self.toolbox.search_files_by_keyword(
                    directory_path=args.get("directory_path"), 
                    keyword=args.get("keyword"),
                    stop_event=stop_event
                )
            elif function_name == "read_file_content":
                return self.toolbox.read_file_content(args.get("file_path"))
            elif function_name == "remember_user_fact":
                return self.toolbox.remember_user_fact(args.get("key"), args.get("value"))
            
            elif function_name == "browse_url":
                return self.toolbox.run_browse_url(args.get("url"))

            # [æ–°å¢] å“¨å…µç³»ç»Ÿå·¥å…·è·¯ç”±
            elif function_name == "add_time_sentinel":
                return self.toolbox.add_time_sentinel(
                    interval=args.get("interval"),
                    unit=args.get("unit"),
                    description=args.get("description")
                )
            elif function_name == "add_file_sentinel":
                return self.toolbox.add_file_sentinel(
                    path=args.get("path"),
                    description=args.get("description")
                )
            elif function_name == "add_behavior_sentinel":
                return self.toolbox.add_behavior_sentinel(
                    key_combo=args.get("key_combo"),
                    description=args.get("description")
                )
            elif function_name == "list_active_sentinels":
                return self.toolbox.list_sentinels()
            elif function_name == "remove_sentinel":
                return self.toolbox.remove_sentinel(args.get("type"), args.get("id"))

            skill_result = self.toolbox.route_skill_tool(function_name, args)
            if skill_result is not None:
                return skill_result
            else:
                return f"Error: Unknown tool '{function_name}'"
        except Exception as e:
            return f"Error executing {function_name}: {str(e)}"

    def clear_memory(self):
        self.memory.clear()
    
    def update_config(self, new_config=None, new_profiles=None, new_active_profile=None):
        """è¿è¡Œæ—¶æ›´æ–°é…ç½®"""
        if new_config:
            with open(self.config_path, 'w', encoding='utf-8') as f:
                yaml.dump(new_config, f, allow_unicode=True, default_flow_style=False)
        
        if new_profiles:
            with open(self.profiles_path, 'w', encoding='utf-8') as f:
                yaml.dump({'profiles': new_profiles}, f, allow_unicode=True, default_flow_style=False)
                
        if new_active_profile:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                current = yaml.safe_load(f)
            current['active_profile'] = new_active_profile
            with open(self.config_path, 'w', encoding='utf-8') as f:
                yaml.dump(current, f, allow_unicode=True, default_flow_style=False)

        self.load_all_configs()
        self._init_client()