# core/host_agent.py
import yaml
import json
import os
import time
import threading
from openai import OpenAI
from core.memory import ConversationMemory
# [ä¿®æ”¹ç‚¹] å¯¼å…¥è§£è€¦åçš„å·¥å…·ç®±
from core.functools.tools import Toolbox
from core.rag_store import RAGStore

class HostAgent:
    def __init__(self, session_id="default", config_path="config/config.yaml"):
        self.session_id = session_id
        
        # è·¯å¾„å®šä¹‰
        self.config_path = config_path
        self.profiles_path = "config/profiles.yaml"
        self.user_profile_path = "config/user_profile.yaml"
        
        # åŠ è½½æ‰€æœ‰é…ç½®
        self.load_all_configs()
        
        # åˆå§‹åŒ–ç»„ä»¶
        # ä» Config è¯»å– window_sizeï¼Œé»˜è®¤ä¸º 10
        win_size = self.config.get('system', {}).get('memory', {}).get('window_size', 10)
        self.memory = ConversationMemory(session_id=self.session_id, window_size=win_size)
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“ (RAG)
        vec_path = self.config.get('system', {}).get('memory', {}).get('vector_store_path', './logs/vector_store')
        self.rag_store = RAGStore(persistence_path=vec_path)
        
        # åˆå§‹åŒ–å·¥å…·ç®±
        self.toolbox = Toolbox(self)
        
        # åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
        self._init_client()

    def load_all_configs(self):
        """åŠ è½½ç³»ç»Ÿé…ç½®ã€æ¨¡å‹é…ç½®å’Œç”¨æˆ·ç”»åƒ"""
        # 1. åŠ è½½ä¸»é…ç½®
        with open(self.config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
            
        # 2. åŠ è½½æ¨¡å‹ Profiles
        if os.path.exists(self.profiles_path):
            with open(self.profiles_path, 'r', encoding='utf-8') as f:
                self.profiles = yaml.safe_load(f).get('profiles', {})
        else:
            self.profiles = {}
            
        # 3. åŠ è½½ç”¨æˆ·ç”»åƒ
        if os.path.exists(self.user_profile_path):
            with open(self.user_profile_path, 'r', encoding='utf-8') as f:
                self.user_data = yaml.safe_load(f)
        else:
            self.user_data = {}

    def _init_client(self):
        """æ ¹æ® active_profile åˆå§‹åŒ– LLM å®¢æˆ·ç«¯"""
        active_id = self.config.get('active_profile')
        
        # å®¹é”™å¤„ç†ï¼šå¦‚æœæ‰¾ä¸åˆ° profileï¼Œä½¿ç”¨é»˜è®¤æˆ–ç©ºé…ç½®
        if active_id not in self.profiles:
            print(f"[Warning] Profile '{active_id}' not found. Using safe defaults.")
            self.current_model_config = {
                'api_key': self.config.get('agent', {}).get('openai_api_key', 'EMPTY'),
                'base_url': self.config.get('agent', {}).get('openai_base_url', None),
                'model': 'gpt-3.5-turbo',
                'temperature': 0.7
            }
        else:
            self.current_model_config = self.profiles[active_id]
            
        self.client = OpenAI(
            api_key=self.current_model_config['api_key'],
            base_url=self.current_model_config.get('base_url')
        )

    def _build_dynamic_system_prompt(self, relevant_memories: list):
        """
        æ„å»ºé«˜çº§ç»“æ„åŒ– Prompt
        åŒ…å«ï¼šèº«ä»½ã€å·¥å…·èƒ½åŠ›ã€ç”¨æˆ·ç”»åƒã€é•¿æœŸè®°å¿†(RAG)ã€å½“å‰å¯¹è¯æ‘˜è¦
        """
        # 1. åŸºç¡€èº«ä»½è®¾å®š
        base_identity = """
Role: Resonance (Advanced AI Host for Windows)
Objective: Assist the user by executing local commands, managing files, and planning complex tasks.
Environment: Windows 11, PowerShell.

Core Principles:
1. **Think Before Acting.** When complex tasks arise, break them down.
2. **Explore then Act.** When asked to find information in files, DO NOT guess. Use 'list_directory_files' or 'search_files_by_keyword' first, then 'read_file_content'.
3. **Multi-Step Tool Use.** You can use multiple tools or use tools multiple times in a sequence to complete a task. Analyze the output of each tool before proceeding.
4. **Robustness.** If a command fails, analyze the error and try a different approach.
5. **Memory.** You have access to long-term memory. Use it to recall user preferences and past projects.
"""
        
        # 2. ç”¨æˆ·ç”»åƒæ³¨å…¥
        user_section = "\n[User Profile & Preferences]\n"
        user_info = self.user_data.get('user_info', {})
        known_projects = self.user_data.get('known_projects', {})
        
        for k, v in user_info.items():
            user_section += f"- {k}: {v}\n"
        if known_projects:
            user_section += "- Known Projects/Paths:\n"
            for proj, path in known_projects.items():
                user_section += f"  * {proj}: {path}\n"

        # 3. é•¿æœŸè®°å¿†æ³¨å…¥ (RAG Results)
        rag_section = ""
        if relevant_memories:
            rag_section = "\n[Relevant Long-term Memories]\n"
            for mem in relevant_memories:
                rag_section += f"- {mem}\n"
            rag_section += "(Use these memories to answer contextually if applicable)\n"

        # 4. å¯¹è¯æ‘˜è¦æ³¨å…¥ (Summary)
        summary_text = self.memory.load_summary()
        summary_section = ""
        if summary_text:
            summary_section = f"\n[Previous Conversation Summary]\n{summary_text}\n(This is what happened before the current active window)\n"

        # ç»„åˆ Prompt
        full_prompt = base_identity + user_section + rag_section + summary_section
        
        return full_prompt

    def _update_summary_if_needed(self):
        """
        [æ‘˜è¦æœºåˆ¶] æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©å†å²è®°å½•
        å¦‚æœå†å²è®°å½•è¶…è¿‡ä¸€å®šé•¿åº¦ï¼Œè°ƒç”¨ LLM ç”Ÿæˆæ‘˜è¦
        """
        if not self.config['system'].get('memory', {}).get('enable_summary', True):
            return

        # ç­–ç•¥ï¼šæ¯éš” 5 è½® (10æ¡æ¶ˆæ¯) æ›´æ–°ä¸€æ¬¡æ‘˜è¦
        full_log = self.memory.get_full_log()
        if len(full_log) > 0 and len(full_log) % 10 == 0:
            # åªæœ‰å½“æœ‰è¶³å¤Ÿå¤šçš„å†å²åœ¨çª—å£ä¹‹å¤–æ—¶æ‰æ€»ç»“
            text_to_summarize = self.memory.get_messages_for_summarization()
            if not text_to_summarize:
                return

            current_summary = self.memory.load_summary()
            
            # ä½¿ç”¨ LLM ç”Ÿæˆæ‘˜è¦
            try:
                prompt = f"""
                You are a memory compressor.
                
                Current Summary:
                {current_summary}
                
                New Conversation Log to Append:
                {text_to_summarize}
                
                Task: Update the summary to include the key information from the new log. Keep it concise.
                Return ONLY the updated summary text.
                """
                
                response = self.client.chat.completions.create(
                    model=self.current_model_config['model'],
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.3
                )
                
                new_summary = response.choices[0].message.content
                self.memory.save_summary(new_summary)
                # print(f"[System]: Memory summarized. Length: {len(new_summary)}")
            except Exception as e:
                print(f"[Warning] Failed to generate summary: {e}")

    # =========================================================================
    # [æ–°å¢æ ¸å¿ƒé€»è¾‘] å¼‚æ­¥è®°å¿†èƒå–ä¸å­˜å‚¨
    # =========================================================================
    def _extract_and_save_memory_async(self, user_input, ai_output):
        """
        åå°çº¿ç¨‹ä»»åŠ¡ï¼šåˆ†æå¯¹è¯ï¼Œèƒå–æœ‰ä»·å€¼çš„ä¿¡æ¯å­˜å…¥å‘é‡åº“ã€‚
        é¿å…å°†åƒåœ¾å¯¹è¯ï¼ˆ"ä½ å¥½", "å—¯"ï¼‰å­˜å…¥ã€‚
        """
        try:
            # 1. å¯å‘å¼è¿‡æ»¤ (Heuristic Filter)
            # å¦‚æœå†…å®¹å¤ªçŸ­ï¼Œé€šå¸¸ä¸å…·å¤‡é•¿æœŸè®°å¿†ä»·å€¼
            if len(user_input) < 5 and len(ai_output) < 10:
                return

            # 2. è°ƒç”¨ LLM è¿›è¡Œä¿¡æ¯èƒå– (Extraction)
            # ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹æˆ–ç›¸åŒçš„æ¨¡å‹ï¼ŒPrompt ä¾§é‡äº"äº‹å®æå–"
            extraction_prompt = f"""
Analyze the following interaction for Long-term Memory storage.
Extract meaningful facts, user preferences, specific project details, or technical solutions.
Ignore casual chitchat (greetings, thanks) or temporary command outputs.

Interaction:
User: {user_input}
AI: {ai_output}

Instructions:
- If the interaction contains useful facts worth remembering for future sessions, extract them into a concise statement.
- If the interaction is trivial or purely operational (e.g. "list files"), output "NO_INFO".
- Do not output "User said...", just the fact.

Output:
"""
            response = self.client.chat.completions.create(
                model=self.current_model_config['model'],
                messages=[{"role": "user", "content": extraction_prompt}],
                temperature=0.1, # ä½æ¸©åº¦ç¡®ä¿å‡†ç¡®
                max_tokens=150
            )
            
            extracted_info = response.choices[0].message.content.strip()
            
            # 3. å­˜å‚¨é€»è¾‘
            if extracted_info and "NO_INFO" not in extracted_info:
                # å­˜å…¥å‘é‡åº“
                success = self.rag_store.add_memory(
                    text=extracted_info,
                    metadata={
                        "type": "conversation_insight",
                        "session": self.session_id,
                        "original_user_input": user_input[:50] # æ–¹ä¾¿è¿½æº¯
                    }
                )
                if success:
                    # åœ¨æ—¥å¿—ä¸­é™é»˜è®°å½•ï¼Œç”¨äºè°ƒè¯•ï¼Œä¸å¹²æ‰°ä¸»çº¿ç¨‹è¾“å‡º
                    # print(f"[Memory System]: Archived -> {extracted_info[:30]}...")
                    pass

        except Exception as e:
            # è¿™é‡Œçš„å¼‚å¸¸ç»å¯¹ä¸èƒ½å½±å“ä¸»çº¿ç¨‹
            print(f"[Memory System Error]: {e}")

    # =========================================================================
    # [æ ¸å¿ƒä¿®æ”¹ç‚¹] æ­£ç¡®å®ç°çš„ ReAct äº¤äº’é€»è¾‘ (å¤šå·¥å…·æ”¯æŒ & å¾ªç¯æ¨ç†)
    # =========================================================================
    def chat(self, user_input, ui_callback=None):
        """
        ä¸»äº¤äº’é€»è¾‘ï¼šéµå¾ª OpenAI å·¥å…·è°ƒç”¨åè®®ï¼Œæ”¯æŒå¤šæ­¥æ€è€ƒã€‚
        """
        try:
            # 1. åˆå§‹åŒ–ä¸Šä¸‹æ–‡
            top_k = self.config['system'].get('memory', {}).get('retrieve_top_k', 3)
            relevant_docs = self.rag_store.search_memory(user_input, n_results=top_k)
            
            # 2. æ„å»ºåŠ¨æ€ System Prompt
            dynamic_sys_prompt = self._build_dynamic_system_prompt(relevant_docs)
            
            # messages åˆ—è¡¨å°†ä½œä¸ºæˆ‘ä»¬åœ¨è¿™ä¸€è½®æ¨ç†ä¸­çš„â€œå·¥ä½œåŒºâ€
            messages = [{"role": "system", "content": dynamic_sys_prompt}]
            messages += self.memory.get_active_context() # è·å–æ»‘åŠ¨çª—å£
            messages.append({"role": "user", "content": user_input})
            
            # 2. å‡†å¤‡å·¥å…·
            tools = self.toolbox.get_tool_definitions()

            # 3. è®°å½•åˆå§‹ç”¨æˆ·æ¶ˆæ¯
            self.memory.add_user_message(user_input)

            # 4. è¿›å…¥ ReAct å¾ªç¯
            max_iterations = 8  # é˜²æ­¢æ¨¡å‹é™·å…¥æ­»å¾ªç¯
            current_iteration = 0
            final_reply = ""

            while current_iteration < max_iterations:
                current_iteration += 1
                
                if ui_callback:
                    ui_callback(f"ğŸ§  Thinking (Step {current_iteration})...")

                # LLM æ¨ç†
                response = self.client.chat.completions.create(
                    model=self.current_model_config['model'],
                    messages=messages,
                    tools=tools,
                    tool_choice="auto",
                    temperature=self.current_model_config['temperature']
                )

                response_message_obj = response.choices[0].message
                
                # [ä¿®æ”¹ç‚¹] å°† OpenAI çš„ Message å¯¹è±¡è½¬æ¢ä¸º dictï¼Œé˜²æ­¢åæœŸå±æ€§è®¿é—®æŠ¥é”™
                # model_dump æ˜¯ Pydantic v2 (openai v1+) çš„æ ‡å‡†å†™æ³•
                response_dict = response_message_obj.model_dump(exclude_none=True)
                
                # å°†è¯¥æ¶ˆæ¯åŠ å…¥æœ¬è½®å¯¹è¯å·¥ä½œä¸Šä¸‹æ–‡
                messages.append(response_dict)
                
                # åŒæ­¥è®°å½•åˆ°æŒä¹…åŒ– Memory
                if response_message_obj.tool_calls:
                    self.memory.add_ai_tool_call(response_message_obj.content, response_message_obj.tool_calls)
                else:
                    self.memory.add_ai_message(response_message_obj.content)

                # B. æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                if not response_message_obj.tool_calls:
                    final_reply = response_message_obj.content
                    break
                
                # C. æ‰§è¡Œå·¥å…·è°ƒç”¨
                tool_calls = response_message_obj.tool_calls
                for tool_call in tool_calls:
                    function_name = tool_call.function.name
                    try:
                        args = json.loads(tool_call.function.arguments)
                    except:
                        args = {}
                    
                    # å›è°ƒ UI
                    if ui_callback:
                        ui_callback(f"âš™ï¸ Executing {function_name}...")
                    
                    # è°ƒç”¨ Toolbox (é€»è¾‘ä¿æŒä¸å˜ï¼Œä½†å¢åŠ ä¸€ä¸ª update_memory çš„ç‰¹æ®Šå¤„ç†)
                    tool_result = self._route_tool_execution(function_name, args, ui_callback)
                    
                    # è®°å½•ç»“æœ
                    self.memory.add_tool_message(tool_result, tool_call.id)
                    
                    # å°†å·¥å…·ç»“æœåŠ å…¥å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡ (role="tool")
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "name": function_name,
                        "content": str(tool_result)
                    })

                # å®Œæˆæœ¬è½®æ‰€æœ‰å·¥å…·æ‰§è¡Œï¼Œç»§ç»­ while å¾ªç¯è®©æ¨¡å‹æ ¹æ® tool ç»“æœè¿›è¡Œä¸‹ä¸€è½®æ€è€ƒ

            # 5. åç½®å¤„ç†é€»è¾‘
            # [ä¿®æ”¹ç‚¹] ä½¿ç”¨ .get('role') è®¿é—®å­—å…¸ï¼Œç¡®ä¿å®‰å…¨
            if not final_reply and len(messages) > 0:
                last_msg = messages[-1]
                if isinstance(last_msg, dict) and last_msg.get('role') == "assistant":
                    final_reply = last_msg.get('content', "")

            self._update_summary_if_needed()
            
            # 8. [ä¿®æ”¹ç‚¹] å¯åŠ¨å¼‚æ­¥çº¿ç¨‹è¿›è¡Œè®°å¿†èƒå–ä¸å‘é‡å­˜å‚¨
            # ä½¿ç”¨å®ˆæŠ¤çº¿ç¨‹ (daemon=True)ï¼Œä¸»ç¨‹åºé€€å‡ºæ—¶å®ƒè‡ªåŠ¨ç»“æŸï¼Œä¸ä¼šå¡æ­»è¿›ç¨‹
            memory_thread = threading.Thread(
                target=self._extract_and_save_memory_async,
                args=(user_input, final_reply),
                daemon=True
            )
            memory_thread.start()

            return final_reply if final_reply else "Task completed (No textual response)."

        except Exception as e:
            import traceback
            print(traceback.format_exc())
            return f"Resonance Core Error: {str(e)}"

    def _route_tool_execution(self, function_name, args, ui_callback):
        """è·¯ç”±å·¥å…·è°ƒç”¨åˆ° Toolboxï¼Œä¿æŒä»£ç æ•´æ´"""
        try:
            if function_name == "invoke_skill" or function_name == "run_registered_script":
                alias = args.get("skill_alias") or args.get("script_alias")
                extra = args.get("args", "")
                return self.toolbox.invoke_registered_skill(alias, extra)
                
            elif function_name == "execute_shell_command":
                return self.toolbox.execute_shell(args.get("command"))
                
            elif function_name == "add_automation_skill":
                return self.toolbox.add_new_script(args.get("alias"), args.get("command"), args.get("description"))
                
            elif function_name == "scan_directory_projects":
                res = self.toolbox.scan_and_remember(args.get("path"))
                self.rag_store.add_memory(res, {"type": "fact_project"})
                return res
                
            elif function_name == "read_file_content":
                return self.toolbox.read_file_content(args.get("file_path"))
                
            elif function_name == "remember_user_fact":
                key = args.get("key")
                val = args.get("value")
                res = self.toolbox.remember_user_fact(key, val)
                self.rag_store.add_memory(f"User Fact: {key} is {val}", {"type": "explicit_fact"})
                return res
                
            elif function_name == "list_directory_files":
                return self.toolbox.list_directory_files(
                    directory_path=args.get("directory_path"), 
                    recursive=args.get("recursive", True),
                    depth=args.get("depth", 2)
                )
                
            elif function_name == "search_files_by_keyword":
                return self.toolbox.search_files_by_keyword(
                    directory_path=args.get("directory_path"), 
                    keyword=args.get("keyword")
                )
            
            else:
                return f"Error: Unknown tool '{function_name}'"
        except Exception as e:
            return f"Error executing {function_name}: {str(e)}"

    def clear_memory(self):
        self.memory.clear()
    
    def update_config(self, new_config=None, new_profiles=None, new_active_profile=None):
        """è¿è¡Œæ—¶æ›´æ–°é…ç½®"""
        if new_config:
            with open(self.config_path, 'w', encoding='utf-8') as f:
                yaml.dump(new_config, f, allow_unicode=True, default_flow_style=False)
        
        if new_profiles:
            with open(self.profiles_path, 'w', encoding='utf-8') as f:
                yaml.dump({'profiles': new_profiles}, f, allow_unicode=True, default_flow_style=False)
                
        if new_active_profile:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                current = yaml.safe_load(f)
            current['active_profile'] = new_active_profile
            with open(self.config_path, 'w', encoding='utf-8') as f:
                yaml.dump(current, f, allow_unicode=True, default_flow_style=False)

        self.load_all_configs()
        self._init_client()